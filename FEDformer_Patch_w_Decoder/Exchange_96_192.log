Args in experiment:
Namespace(L=3, activation='gelu', base='legendre', batch_size=32, c_out=8, checkpoints='./checkpoints/', cross_activation='tanh', d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='exchange_rate.csv', dec_in=8, des='Exp', detail_freq='h', devices='0,1', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', enc_in=8, factor=3, features='M', freq='h', gpu=0, is_training=1, itr=1, label_len=48, learning_rate=0.0001, loss='mse', lradj='type1', mode_select='random', model='FEDformer_Patch_w_Decoder', modes=64, moving_avg=[24], n_heads=8, num_workers=10, output_attention=False, patch_len=16, patience=3, pred_len=192, root_path='./dataset/exchange_rate/', seq_len=96, stride=8, target='OT', task_id='Exchange', train_epochs=10, use_amp=False, use_gpu=True, use_multi_gpu=False, version='Fourier')
Use GPU: cuda:0
PatchEmbedding: seq_len=96, patch_len=16, stride=8
PatchEmbedding: num_patch=11, pad_len=0
PatchEmbedding: seq_len=240, patch_len=16, stride=8
PatchEmbedding: num_patch=29, pad_len=0
w/ Decoder Model: enc_patches=11, dec_patches=29
fourier enhanced block used!
modes=5, index=[0, 1, 2, 3, 4]
fourier enhanced block used!
modes=14, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
 fourier enhanced cross attention used!
modes_q=14, index_q=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
modes_kv=5, index_kv=[0, 1, 2, 3, 4]
>>>>>>>start training : Exchange_FEDformer_Patch_w_Decoder_random_modes64_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 5024
val 569
test 1326
Epoch: 1 cost time: 7.478237628936768
Epoch: 1, Steps: 157 | Train Loss: 0.6544220 Vali Loss: 1.0497650 Test Loss: 0.4654057
Validation loss decreased (inf --> 1.049765).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 6.551962375640869
Epoch: 2, Steps: 157 | Train Loss: 0.3275972 Vali Loss: 0.5910411 Test Loss: 0.2309745
Validation loss decreased (1.049765 --> 0.591041).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 6.551519870758057
Epoch: 3, Steps: 157 | Train Loss: 0.2988988 Vali Loss: 0.5282404 Test Loss: 0.2096763
Validation loss decreased (0.591041 --> 0.528240).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 6.5862109661102295
Epoch: 4, Steps: 157 | Train Loss: 0.2922180 Vali Loss: 0.5170426 Test Loss: 0.2078815
Validation loss decreased (0.528240 --> 0.517043).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 7.341951847076416
Epoch: 5, Steps: 157 | Train Loss: 0.2891657 Vali Loss: 0.5262008 Test Loss: 0.2070391
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 7.331408739089966
Epoch: 6, Steps: 157 | Train Loss: 0.2876854 Vali Loss: 0.5121899 Test Loss: 0.2047610
Validation loss decreased (0.517043 --> 0.512190).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 7.138761758804321
Epoch: 7, Steps: 157 | Train Loss: 0.2868624 Vali Loss: 0.5085350 Test Loss: 0.2041123
Validation loss decreased (0.512190 --> 0.508535).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 7.07781457901001
Epoch: 8, Steps: 157 | Train Loss: 0.2863619 Vali Loss: 0.5078455 Test Loss: 0.2036960
Validation loss decreased (0.508535 --> 0.507846).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9 cost time: 6.604762315750122
Epoch: 9, Steps: 157 | Train Loss: 0.2861851 Vali Loss: 0.5065575 Test Loss: 0.2036200
Validation loss decreased (0.507846 --> 0.506557).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10 cost time: 6.580773830413818
Epoch: 10, Steps: 157 | Train Loss: 0.2860236 Vali Loss: 0.5067936 Test Loss: 0.2036213
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.953125e-07
>>>>>>>testing : Exchange_FEDformer_Patch_w_Decoder_random_modes64_custom_ftM_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 1326
test shape: (41, 32, 192, 8) (41, 32, 192, 8)
test shape: (1312, 192, 8) (1312, 192, 8)
mse:0.20361998677253723, mae:0.34410205483436584
